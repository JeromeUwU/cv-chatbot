import os, re, io, json, hashlib
import streamlit as st
from groq import Groq
from dotenv import load_dotenv

# Initialisation

load_dotenv()
st.set_page_config(page_title="CV Chatbot ¬∑ J√©r√¥me TAM", page_icon="üíº", layout="centered")

GROQ_API_KEY = st.secrets.get("GROQ_API_KEY") or os.getenv("GROQ_API_KEY")
if not GROQ_API_KEY:
    st.error("Cl√© API manquante. Ajoute GROQ_API_KEY dans les secrets.")
    st.stop()

client = Groq(api_key=GROQ_API_KEY)
DEFAULT_MODEL = "llama-3.3-70b-versatile"
CV_TEXT = st.secrets.get("CV_TEXT", "").strip()

MAX_QUESTIONS = 10  




# System prompt 
def build_system_prompt(cv_text: str, style: str) -> str:
    cv_snippet = cv_text[:6000]  # limite tokens
    base = f"""
Tu joues le r√¥le du candidat dont le CV suit, et tu r√©ponds comme en entretien.
- Langue: si l'interlocuteur parle fran√ßais ‚Üí r√©ponds en **je** en fran√ßais naturel; sinon en anglais avec **I**.
- Ton: bref, chaleureux, professionnel; phrases naturelles; pas de jargon inutile; 3‚Äì6 bullets max si utile.
- Quand on te demande "peux-tu faire X ?" :
  1) Si l'information est **clairement pr√©sente** dans le CV ‚Üí r√©ponds **oui**/**non** simplement, puis illustre en 2‚Äì4 points concrets tir√©s du CV (exp√©riences, projets, outils).
  2) Si **ce n'est pas mentionn√©** dans le CV ‚Üí dis-le simplement: "D‚Äôapr√®s mon CV, je n‚Äôai pas encore pratiqu√© X."
     - Ensuite, fais un **pont cr√©dible**: "En revanche, j‚Äôai [comp√©tence/formation voisine Y] qui s‚Äôen rapproche (par ex. ...)."
     - Conclus humainement : "Je peux monter vite en comp√©tence" (+ 1 phrase sur ta d√©marche: apprendre vite, documenter, demander un cadrage, livrer un POC).
  3) Si la question est trop vague ‚Üí pose **une seule** question de clarification.
- Interdits: niveaux de confiance, scores invent√©s, exp√©riences non pr√©sentes; pas de phrases type "Verdict", "Confiance", etc.
- Pr√©f√®re le **premier degr√© humain**: "Oui, parce que‚Ä¶" / "Pas encore, mais‚Ä¶" / "Voil√† comment je m‚Äôy prends‚Ä¶"

CV (verbatim) :
---
{cv_snippet}
---

# Gabarits de r√©ponses (FR)

## Comp√©tence clairement pr√©sente
"Oui, sur ce point je suis √† l‚Äôaise.
- [Preuve 1 tir√©e du CV]
- [Preuve 2 tir√©e du CV]
- [Outil/techno pertinente]
Si besoin, je peux vous d√©tailler la d√©marche / un exemple concret."

## Comp√©tence non mentionn√©e dans le CV (pont formation/bases)
"D‚Äôapr√®s mon CV, je n‚Äôai pas encore pratiqu√© X directement.
En revanche, j‚Äôai des bases qui s‚Äôen rapprochent :
- [Module/projet/outil voisin 1]
- [Module/projet/outil voisin 2]
Je peux monter vite en comp√©tence : je commence par un petit cadrage, un POC mesurable, puis j‚Äôindustrialise si √ßa valide."

## Question trop vague
"Pour √™tre pr√©cis, vous pensez √† quel contexte pour X ? (ex : outil attendu, volume de donn√©es, d√©lai)"
"""
    prefix = "R√©ponds en fran√ßais, √† la premi√®re personne (je).\n" if style == "pro (FR)" else "Answer in English, first person (I).\n"
    return prefix + base





# UI ‚Äî Sidebar
st.sidebar.title("‚öôÔ∏è Options")

sys_style = st.sidebar.selectbox("Style de r√©ponse", ["pro (FR)", "pro (EN)"], index=0)


#  Init session
def user_count() -> int:
    return sum(1 for m in st.session_state.messages if m["role"] == "user")

if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "system", "content": build_system_prompt(CV_TEXT, sys_style)}]
    st.session_state.blocked = False  

# Rebuild prompt si style change
if st.session_state.get("sys_style") != sys_style:
    st.session_state.messages[0] = {"role": "system", "content": build_system_prompt(CV_TEXT, sys_style)}
    st.session_state.sys_style = sys_style

# √âtat blocage dur 
asked = user_count()
if asked >= MAX_QUESTIONS:
    st.session_state.blocked = True


#  En-t√™te + compteur + verrou

st.title("üíº CV ‚Äî J√©r√¥me TAM")
st.caption("Limite stricte √† 10 questions par session.")

remaining = max(0, MAX_QUESTIONS - asked)
st.info(f"Questions restantes : **{remaining}/{MAX_QUESTIONS}**")
st.progress(asked / MAX_QUESTIONS if MAX_QUESTIONS else 0.0)

if st.session_state.blocked:
    st.error("Limite atteinte. Le chat est verrouill√© pour cette session.")
    disabled_all = True
else:
    disabled_all = False


# Historique 

for m in st.session_state.messages:
    if m["role"] == "system":
        continue
    with st.chat_message("user" if m["role"] == "user" else "assistant"):
        st.markdown(m["content"])


# Suggestions (d√©sactiv√©es si blocage)

with st.expander("üí° Suggestions", expanded=True):
    cols = st.columns(3)
    examples = [
        "Peux-tu r√©sumer le profil de J√©r√¥me en 3 phrases ?",
        "Pr√©pare 5 bullets pour 'Parlez-moi de vous'.",
        "R√©ponses types : forces, faiblesses, pr√©tentions salariales.",
        "Donne 3 exemples STAR (Situation/T√¢che/Action/R√©sultat).",
        "Pitch FR puis traduction EN.",
        "Liste des projets concrets de J√©r√¥me (avec m√©triques).",
    ]
    for i, ex in enumerate(examples):
        if cols[i % 3].button(ex, use_container_width=True, key=f"sugg_{i}", disabled=disabled_all):
            st.session_state.user_prefill = ex


# Groq streaming

def stream_groq(messages, model_name):
    resp = client.chat.completions.create(
        model=model_name,
        messages=messages,
        temperature=0.4,   
        stream=True,
    )
    for chunk in resp:
        if chunk.choices:
            delta = chunk.choices[0].delta
            if delta and getattr(delta, "content", None):
                yield delta.content


#  Input (verrouill√© si blocage)

placeholder = "Pose une question li√©e au profil (comp√©tences, exp√©riences, soft skills, MLOps, salaire, dispo)‚Ä¶"
prompt = st.chat_input(placeholder=placeholder, key="chat_input", disabled=disabled_all)

if prompt is None and "user_prefill" in st.session_state and not disabled_all:
    prompt = st.session_state.pop("user_prefill")


#  Gate & ex√©cution 

if prompt:

    if user_count() >= MAX_QUESTIONS or st.session_state.blocked:
        st.session_state.blocked = True
        with st.chat_message("assistant"):
            st.error("Limite atteinte. Le chat est verrouill√© pour cette session.")
    else:
            
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            stream_area = st.empty()
            chunks = stream_groq(st.session_state.messages, DEFAULT_MODEL)
            full = stream_area.write_stream(chunks)

        st.session_state.messages.append({"role": "assistant", "content": full})

            
        if user_count() >= MAX_QUESTIONS:
            st.session_state.blocked = True
            st.toast("‚ö†Ô∏è Limite atteinte : le chat est d√©sormais verrouill√©.")