import os, re, io, json, hashlib
import streamlit as st
from groq import Groq
from dotenv import load_dotenv

# Initialisation

load_dotenv()
st.set_page_config(page_title="CV Chatbot ¬∑ J√©r√¥me TAM", page_icon="üíº", layout="centered")

GROQ_API_KEY = st.secrets.get("GROQ_API_KEY") or os.getenv("GROQ_API_KEY")
if not GROQ_API_KEY:
    st.error("Cl√© API manquante. Ajoute GROQ_API_KEY dans les secrets.")
    st.stop()

client = Groq(api_key=GROQ_API_KEY)
DEFAULT_MODEL = "llama-3.3-70b-versatile"
CV_TEXT = st.secrets.get("CV_TEXT", "").strip()

MAX_QUESTIONS = 10  



# Th√®mes g√©n√©raux d'entretien (FR/EN)
TECH_WHITELIST = {
    
    "cv","profil","exp√©rience","experiences","projet","projets","comp√©tence","competences","skills",
    "salaire","pr√©tentions","pretentions","forces","faiblesses","disponibilit√©","mobilit√©","education",
    "formation","langues","github","linkedin","portfolio","dispo","contrat","stage","alternance",
    "freelance","cdi","cdd","mission",
   
    "groq","streamlit","fastapi","aws","ec2","s3","rds","docker","pytorch","xgboost","lightgbm","prophet",
    "arima","sql","postgresql","llm","rag","lora","quantization","vision","vit","conformer","wav2vec",
    "vq","gumbel","diffusion","unet","kalalodata","activus","rsna","recipeprep","app","ios","kaggle",
    "weaviate","langchain","langgraph","mops","mlops","we","w&b","weights","biases","power bi","tableau"
}

GENERAL_INTERVIEW_TERMS = {
    "soft skills","culture fit","work style","under pressure","teamwork","communication","leadership","ownership",
    "problem solving","problem-solving","conflict","deadline","stress","adaptability","autonomy","collaboration",
    "motivation","learning","feedback","prioritization","time management","stakeholder","mentoring","pair programming","team","work",
    
    "sous pression","travail en √©quipe","communication","leadership","probl√®me","r√©solution de probl√®me","conflit",
    "√©ch√©ance","stress","adaptabilit√©","autonomie","collaboration","motivation","apprentissage","priorisation",
    "gestion du temps","parties prenantes","encadrement","mentorat","culture","valeurs","√©thique","√©quipe","equipe","travail",
    "temps","gestion","proposition","esprit","integration","Pitch","pitch","candidat","Candidat","Jerome","J√©r√¥me","Peut-il","peut-il",
    "peut il","peut","est ce que","sait il","sait",'problem'
}


TECH_CAPABILITY_TERMS = {
    "mlops","pipeline","ml pipeline","workflow","cicd","monitoring","observability","testing","unit tests",
    "design","architecture","scalability","security","privacy","data quality","feature store",
    "serving","deployment","inference","latency","cost optimization",
  
    "cha√Æne ml","cha√Æne de ml","d√©ploiement","surveillance","observabilit√©","tests","architecture","scalabilit√©",
    "s√©curit√©","confidentialit√©","qualit√© des donn√©es","magasin de features","serving","inf√©rence","latence","co√ªt"
}


SOFT_SKILL_PATTERNS = [
    r"bon\s+.*sous\s+pression", r"work(ing)?\s+under\s+pressure",
    r"(soft\s*skills?|comp√©tences\s+comportementales?)",
    r"communication|communicant|communication skills",
    r"(team|√©quipe).*(player|travail)", r"leadership",
    r"(gestion|management)\s+du\s+temps|time\s+management",
    r"r√©solution\s+de\s+probl(√®|e)me|problem[-\s]*solving",
    r"adaptabilit(√©|e)|adaptability", r"motivation|motivated",
]


def extract_keywords(text: str) -> set:
    words = re.findall(r"[A-Za-z√Ä-√ñ√ò-√∂√∏-√ø0-9\-\+]{3,}", text.lower())
    freq = {}
    for w in words:
        if w.isdigit():
            continue
        freq[w] = freq.get(w, 0) + 1
    top = {w for (w, c) in freq.items() if c >= 2}
    return top

CV_TERMS = extract_keywords(CV_TEXT)

ALLOWED_TERMS = (
    set(w.lower() for w in TECH_WHITELIST)
    | set(w.lower() for w in GENERAL_INTERVIEW_TERMS)
    | set(w.lower() for w in TECH_CAPABILITY_TERMS)
    | CV_TERMS
)

def always_allow(query: str) -> bool:
    q = query.lower()
   
    for pat in SOFT_SKILL_PATTERNS:
        if re.search(pat, q):
            return True
 
    if ("candidat" in q or "candidate" in q) and any(k in q for k in [
        "bon","good","peut","can","sait","know","capable","able","comp√©tent","competent"
    ]):
        return True

    toks = set(re.findall(r"[A-Za-z√Ä-√ñ√ò-√∂√∏-√ø0-9\-\+]{3,}", q))
    if toks & (GENERAL_INTERVIEW_TERMS | TECH_CAPABILITY_TERMS):
        return True
    return False

GATE_THRESHOLD = 0.05

def is_on_topic(query: str, threshold: float = GATE_THRESHOLD) -> bool:
    if always_allow(query):
        return True
    q_tokens = set(re.findall(r"[A-Za-z√Ä-√ñ√ò-√∂√∏-√ø0-9\-\+]{3,}", query.lower()))
    if not q_tokens:
        return False
    overlap = len(q_tokens & ALLOWED_TERMS) / max(1, len(q_tokens))
    return overlap >= threshold



# System prompt 
def build_system_prompt(cv_text: str, style: str) -> str:
    cv_snippet = cv_text[:6000]  # limiter les tokens
    base = f"""
Tu es un assistant d'entretien qui r√©pond STRICTEMENT √† partir du CV ci-dessous (exp√©riences, projets, **formation**).
Langue: FR si l'utilisateur parle FR; sinon EN. Style concis, concret, structur√© (bullets OK).

CV du candidat (verbatim) :
---
{cv_snippet}
---

# POLITIQUE DE R√âPONSE
1) On-topic uniquement : profil, comp√©tences, exp√©riences, projets, forces/faiblesses, salaire, dispo, soft skills, MLOps.
2) Z√©ro hallucination : ne pas inventer d'outils/exp√©riences absents du CV.
3) Hi√©rarchie des preuves (dans cet ordre) :
   A. Exp√©riences professionnelles / projets livr√©s
   B. Projets personnels / comp√©titions (Kaggle, GitHub)
   C. **Formation (cours/TP/outils)** ‚Üí autoris√©e pour **inf√©rer** une capacit√© si pas explicitement mentionn√©e ailleurs.
4) Questions de type "Le candidat sait-il faire X ?" :
   a) Si preuve explicite (A/B) ‚Üí **Verdict : Oui/Non** + 2‚Äì3 puces d'√©vidence (citations du CV).
   b) Si pas explicite mais **formation pertinente (C)** :
      ‚Ä¢ Dis-le : "*Non mentionn√© explicitement dans l'exp√©rience.*"
      ‚Ä¢ **Fais le pont formation‚Üícomp√©tence** : "*Cependant, la formation en [module/TP/outil] est √©troitement li√©e √† X (raisons 1‚Äì2).*"
      ‚Ä¢ Donne un **niveau de confiance** (√âlev√©/Moyen/Faible) selon la proximit√© formation‚ÜîX.
      ‚Ä¢ Propose 1 **question de clarification** ou 3 **√©tapes d'action**.
   c) **Mont√©e en comp√©tence (si pertinent)** :
      ‚Ä¢ Mentionne que **les bases acquises en formation** (ex. stats avanc√©es, GLM, processus stochastiques, optimisation, RO/Xpress, EDP/FreeFEM, signal & image sans DL, POO Python/C++) **+ les qualit√©s list√©es dans ‚ÄúForces‚Äù** (ex. apprentissage rapide, adaptabilit√©, rigueur) **soutiennent une mont√©e en comp√©tence rapide** sur X.
      ‚Ä¢ Formulation attendue : "*Non indiqu√© en exp√©rience. **Appui formation** : [bases pr√©cises]. **Qualit√©s** : [2‚Äì3 forces]. ‚áí **Mont√©e en comp√©tence rapide** attendue sur X.*"
   d) Si totalement hors champ ‚Üí refuse poliment et propose de reformuler.
5) Format recommand√© :
   - **Verdict** : Oui / Probable (inf√©rence) / Non indiqu√©
   - **Pourquoi** : 2‚Äì4 puces (mapping besoin X ‚Üî √©l√©ments du CV ou **Formation**)
   - **Appui formation** (si utilis√©) : 1‚Äì2 puces citant explicitement les modules/outils (ex. *RO/Xpress*, *EDP/FreeFEM*, *traitement du signal (convolutions/d√©bruitage)*, *stats avanc√©es (bay√©sien/GLM/Kalman)*)
   - **Mont√©e en comp√©tence** (si utilis√©) : 1 puce rappelant **bases** + **qualit√©s** ‚Üí mont√©e en comp√©tence rapide
   - **Confiance** : √âlev√© / Moyen / Faible
   - **Next step** : 1 question de pr√©cision OU 3 √©tapes concr√®tes

# EXEMPLES DE TON (FR)
- "Verdict : Probable (inf√©rence). Pourquoi : CI/CD + Docker + FastAPI + AWS vus en projet ‚Üí √©l√©ments d'une pipeline MLOps. **Appui formation** : optimisation/ML math + validation crois√©e. **Mont√©e en comp√©tence** : bases solides + apprentissage rapide ‚áí ramp-up rapide. Confiance : √âlev√©. Next step : pr√©ciser l'outillage de monitoring (logs/metrics/alerts)."
- "Non indiqu√© dans l'exp√©rience. **Appui formation** : *stats avanc√©es (bay√©sien/GLM), processus stochastiques, th√©orie de la mesure* ‚áí bases actuariat. **Mont√©e en comp√©tence** : rigueur + adaptabilit√© ‚áí ramp-up rapide sur tarification (fr√©quence/co√ªt). Confiance : Moyen. Next step : outils cibles (SAS/R/Python, stack BI) ?"
"""
    prefix = "R√©ponds en fran√ßais professionnel.\n" if style == "pro (FR)" else "Reply in professional English.\n"
    return prefix + base




# UI ‚Äî Sidebar
st.sidebar.title("‚öôÔ∏è Options")

model = st.sidebar.selectbox(
    "Mod√®le",
    [DEFAULT_MODEL, "llama-3.3-8b-instant"],
    index=0,
)
sys_style = st.sidebar.selectbox("Style de r√©ponse", ["pro (FR)", "pro (EN)"], index=0)


#  Init session
def user_count() -> int:
    return sum(1 for m in st.session_state.messages if m["role"] == "user")

if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "system", "content": build_system_prompt(CV_TEXT, sys_style)}]
    st.session_state.blocked = False  

# Rebuild prompt si style change
if st.session_state.get("sys_style") != sys_style:
    st.session_state.messages[0] = {"role": "system", "content": build_system_prompt(CV_TEXT, sys_style)}
    st.session_state.sys_style = sys_style

# √âtat blocage dur 
asked = user_count()
if asked >= MAX_QUESTIONS:
    st.session_state.blocked = True


#  En-t√™te + compteur + verrou

st.title("üíº CV ‚Äî J√©r√¥me TAM")
st.caption("Streamlit + Groq (stream). Limite stricte √† 10 questions par session.")

remaining = max(0, MAX_QUESTIONS - asked)
st.info(f"Questions restantes : **{remaining}/{MAX_QUESTIONS}**")
st.progress(asked / MAX_QUESTIONS if MAX_QUESTIONS else 0.0)

if st.session_state.blocked:
    st.error("Limite atteinte. Le chat est verrouill√© pour cette session.")
    disabled_all = True
else:
    disabled_all = False


# Historique 

for m in st.session_state.messages:
    if m["role"] == "system":
        continue
    with st.chat_message("user" if m["role"] == "user" else "assistant"):
        st.markdown(m["content"])


# Suggestions (d√©sactiv√©es si blocage)

with st.expander("üí° Suggestions", expanded=True):
    cols = st.columns(3)
    examples = [
        "Peux-tu r√©sumer le profil de J√©r√¥me en 3 phrases ?",
        "Pr√©pare 5 bullets pour 'Parlez-moi de vous'.",
        "R√©ponses types : forces, faiblesses, pr√©tentions salariales.",
        "Donne 3 exemples STAR (Situation/T√¢che/Action/R√©sultat).",
        "Pitch FR puis traduction EN.",
        "Liste des projets concrets de J√©r√¥me (avec m√©triques).",
        "Expertise de J√©r√¥me (simplifi√©)."
    ]
    for i, ex in enumerate(examples):
        if cols[i % 3].button(ex, use_container_width=True, key=f"sugg_{i}", disabled=disabled_all):
            st.session_state.user_prefill = ex


# Groq streaming

def stream_groq(messages, model_name):
    resp = client.chat.completions.create(
        model=model_name,
        messages=messages,
        temperature=0.2,
        stream=True,
    )
    for chunk in resp:
        if chunk.choices:
            delta = chunk.choices[0].delta
            if delta and getattr(delta, "content", None):
                yield delta.content


#  Input (verrouill√© si blocage)

placeholder = "Pose une question li√©e au profil (comp√©tences, exp√©riences, soft skills, MLOps, salaire, dispo)‚Ä¶"
prompt = st.chat_input(placeholder=placeholder, key="chat_input", disabled=disabled_all)

if prompt is None and "user_prefill" in st.session_state and not disabled_all:
    prompt = st.session_state.pop("user_prefill")


#  Gate & ex√©cution 

if prompt:

    if user_count() >= MAX_QUESTIONS or st.session_state.blocked:
        st.session_state.blocked = True
        with st.chat_message("assistant"):
            st.error("Limite atteinte. Le chat est verrouill√© pour cette session.")
    else:
      
        if not is_on_topic(prompt):
            with st.chat_message("assistant"):
                st.info("Je r√©ponds uniquement aux questions li√©es au **profil du candidat** "
                        "(comp√©tences, exp√©riences, soft skills, MLOps, salaire, disponibilit√©).")
        else:
            
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)

            with st.chat_message("assistant"):
                stream_area = st.empty()
                chunks = stream_groq(st.session_state.messages, model)
                full = stream_area.write_stream(chunks)

            st.session_state.messages.append({"role": "assistant", "content": full})

            
            if user_count() >= MAX_QUESTIONS:
                st.session_state.blocked = True
                st.toast("‚ö†Ô∏è Limite atteinte : le chat est d√©sormais verrouill√©.")